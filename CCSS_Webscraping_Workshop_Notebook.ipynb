{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "u2LF-NeB49ek",
        "L7_zuZsp67qy",
        "lrrEfjAt79Dc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CCSS Webscraping Workshop- Fall 2022\n",
        "*CCSS Data Science Fellow Remy Stewart*\n",
        "\n",
        "Welcoming to the coding demonstrations of the CCSS Web Scraping Fall 2022 workshop! \n",
        "\n",
        "The first module will introduce the fundamentals around interacting with websites to retrieve data by highlighting the Beautiful Soup library. The second module will explore how to navigate dynamically generated website content through interactive scraping powered by Selenium.\n",
        "\n",
        "We'll be using WebScraper.io's [webscraping test site](https://webscraper.io/test-sites/e-commerce/static) structured as an  electronics e-commerce site. This webpage serves as a great testing ground to learn about webscraping without having to worry about site blockages when building our initial scraper. We'll be toggling back and forth between this notebook and the website to use its Inspect feature to learn more about the site's underlying structure throughout the demos. \n"
      ],
      "metadata": {
        "id": "q8L6UD2DawPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Module 1- Fundamentals with Beautiful Soup \n",
        "\n",
        "Let's start by importing in the libraries we'll be using within the first module: "
      ],
      "metadata": {
        "id": "1nuAabhF4V7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "import re\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "cJgBYEg24VLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Our initial goal will be to retrieving the name, price, description, and number of reviews of the three items featured on the site home page. `urllib.requests` is a library that goes hand-in-hand with Beautiful Soup by establishing the HTTP connection we'll need with the e-commerce site to parse the site's HTML. Let's start by inspecting the full HTML of the site itself by directly using the `BeautifulSoup` method with a designated `html.parser`:"
      ],
      "metadata": {
        "id": "fVMGJUMgeOKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "html = urlopen('https://webscraper.io/test-sites/e-commerce/static')\n",
        "bs_html = BeautifulSoup(html, 'html.parser')\n",
        "print(bs_html)"
      ],
      "metadata": {
        "id": "fGx03N9H45nu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f52b41-266f-4421-a902-a45b339fe50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "<!-- Anti-flicker snippet (recommended)  -->\n",
            "<style>.async-hide {\n",
            "\t\topacity: 0 !important\n",
            "\t} </style>\n",
            "<script>(function (a, s, y, n, c, h, i, d, e) {\n",
            "\t\ts.className += ' ' + y;\n",
            "\t\th.start = 1 * new Date;\n",
            "\t\th.end = i = function () {\n",
            "\t\t\ts.className = s.className.replace(RegExp(' ?' + y), '')\n",
            "\t\t};\n",
            "\t\t(a[n] = a[n] || []).hide = h;\n",
            "\t\tsetTimeout(function () {\n",
            "\t\t\ti();\n",
            "\t\t\th.end = null\n",
            "\t\t}, c);\n",
            "\t\th.timeout = c;\n",
            "\t})(window, document.documentElement, 'async-hide', 'dataLayer', 4000,\n",
            "\t\t{'GTM-NVFPDWB': true});</script>\n",
            "<!-- Google Tag Manager -->\n",
            "<script>(function (w, d, s, l, i) {\n",
            "\t\tw[l] = w[l] || [];\n",
            "\t\tw[l].push({\n",
            "\t\t\t'gtm.start':\n",
            "\t\t\t\tnew Date().getTime(), event: 'gtm.js'\n",
            "\t\t});\n",
            "\t\tvar f = d.getElementsByTagName(s)[0],\n",
            "\t\t\tj = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : '';\n",
            "\t\tj.async = true;\n",
            "\t\tj.src =\n",
            "\t\t\t'https://www.googletagmanager.com/gtm.js?id=' + i + dl;\n",
            "\t\tf.parentNode.insertBefore(j, f);\n",
            "\t})(window, document, 'script', 'dataLayer', 'GTM-NVFPDWB');</script>\n",
            "<!-- End Google Tag Manager -->\n",
            "<title>Web Scraper Test Sites</title>\n",
            "<meta charset=\"utf-8\"/>\n",
            "<meta content=\"IE=edge,chrome=1\" http-equiv=\"X-UA-Compatible\"/>\n",
            "<meta content=\"web scraping,Web Scraper,Chrome extension,Crawling,Cross platform scraper\" name=\"keywords\">\n",
            "<meta content=\"The most popular web scraping extension. Start scraping in minutes. Automate your tasks with our Cloud Scraper. No software to download, no coding needed.\" name=\"description\">\n",
            "<link href=\"/favicon.png\" rel=\"icon\" sizes=\"128x128\"/>\n",
            "<meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
            "<link href=\"/css/app.css?id=e30a09b05325a08d79dd\" rel=\"stylesheet\"/>\n",
            "<link href=\"https://webscraper.io/test-sites/e-commerce/static\" rel=\"canonical\"/>\n",
            "<link href=\"/img/logo-icon.png\" rel=\"apple-touch-icon\"/>\n",
            "<script defer=\"\" src=\"/js/app.js?id=1c1cafc4a642f71eb469\"></script>\n",
            "</meta></meta></head>\n",
            "<body>\n",
            "<!-- Google Tag Manager (noscript) -->\n",
            "<noscript>\n",
            "<iframe height=\"0\" src=\"https://www.googletagmanager.com/ns.html?id=GTM-NVFPDWB\" style=\"display:none;visibility:hidden\" width=\"0\"></iframe>\n",
            "</noscript>\n",
            "<!-- End Google Tag Manager (noscript) -->\n",
            "<header class=\"navbar navbar-fixed-top navbar-static\" role=\"banner\">\n",
            "<div class=\"container\">\n",
            "<div class=\"navbar-header\">\n",
            "<a data-target=\".side-collapse\" data-target-2=\".side-collapse-container\" data-toggle=\"collapse-side\">\n",
            "<button aria-controls=\"navbar\" aria-expanded=\"false\" class=\"navbar-toggle pull-right collapsed\" data-target=\"#navbar\" data-target-2=\".side-collapse-container\" data-target-3=\".side-collapse\" data-toggle=\"collapse\" type=\"button\">\n",
            "<span class=\"sr-only\">Toggle navigation</span>\n",
            "<span class=\"icon-bar top-bar\"></span>\n",
            "<span class=\"icon-bar middle-bar\"></span>\n",
            "<span class=\"icon-bar bottom-bar\"></span>\n",
            "</button>\n",
            "</a>\n",
            "<div class=\"navbar-brand\">\n",
            "<a href=\"/\"><img alt=\"Web Scraper\" src=\"/img/logo_white.svg\"/></a>\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"side-collapse in\">\n",
            "<nav class=\"navbar-collapse collapse\" id=\"navbar\" role=\"navigation\">\n",
            "<ul class=\"nav navbar-nav navbar-right\">\n",
            "<li class=\"hidden\">\n",
            "<a href=\"#page-top\"></a>\n",
            "</li>\n",
            "<li>\n",
            "<a class=\"menuitm\" href=\"/\">\n",
            "<p>Web Scraper</p>\n",
            "<div class=\"crta\"></div>\n",
            "</a>\n",
            "</li>\n",
            "<li>\n",
            "<a class=\"menuitm\" href=\"/cloud-scraper\">\n",
            "<p>Cloud Scraper</p>\n",
            "<div class=\"crta\"></div>\n",
            "</a>\n",
            "</li>\n",
            "<li>\n",
            "<a class=\"menuitm\" href=\"/pricing\">\n",
            "<p>Pricing</p>\n",
            "<div class=\"crta\"></div>\n",
            "</a>\n",
            "</li>\n",
            "<li class=\"dropdown\">\n",
            "<a class=\"menuitm dropdown-toggle\" data-toggle=\"dropdown\" href=\"#section3\">\n",
            "<p>Learn</p>\n",
            "<div class=\"crta\"></div>\n",
            "</a>\n",
            "<ul class=\"dropdown-menu\">\n",
            "<li>\n",
            "<a href=\"/documentation\">Documentation</a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"/tutorials\">Video Tutorials</a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"/how-to-videos\">How to</a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"/test-sites\">Test Sites</a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"https://forum.webscraper.io/\" rel=\"noopener\" target=\"_blank\">Forum</a>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<a class=\"btn-menu1 install-extension\" href=\"https://chrome.google.com/webstore/detail/web-scraper/jnhgnonknehpejjnehehllkliplmbmhn?hl=en\" rel=\"noopener\" target=\"_blank\">Install</a>\n",
            "</li>\n",
            "<li>\n",
            "<a class=\"btn-menu2\" href=\"https://cloud.webscraper.io/\">Login</a>\n",
            "</li>\n",
            "</ul>\n",
            "</nav>\n",
            "</div>\n",
            "</div>\n",
            "</header>\n",
            "<div class=\"wrapper\">\n",
            "<div class=\"formenu-here container-fluid\">\n",
            "</div>\n",
            "<div class=\"container-fluid blog-hero\">\n",
            "<div class=\"container\">\n",
            "<div class=\"row\">\n",
            "<div class=\"col-md-12\">\n",
            "<h1>Test Sites</h1>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"container test-site\">\n",
            "<div class=\"row\">\n",
            "<div class=\"col-md-3 sidebar\">\n",
            "<div class=\"navbar-default sidebar\" role=\"navigation\">\n",
            "<div class=\"sidebar-nav navbar-collapse\">\n",
            "<ul class=\"nav\" id=\"side-menu\">\n",
            "<li class=\"active\">\n",
            "<a href=\"/test-sites/e-commerce/static\">Home</a>\n",
            "</li>\n",
            "<li>\n",
            "<a class=\"category-link \" href=\"/test-sites/e-commerce/static/computers\">\n",
            "\t\t\t\t\tComputers\n",
            "\t\t\t\t\t<span class=\"fa arrow\"></span>\n",
            "</a>\n",
            "</li>\n",
            "<li>\n",
            "<a class=\"category-link \" href=\"/test-sites/e-commerce/static/phones\">\n",
            "\t\t\t\t\tPhones\n",
            "\t\t\t\t\t<span class=\"fa arrow\"></span>\n",
            "</a>\n",
            "</li>\n",
            "</ul>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"col-md-9\">\n",
            "<div class=\"jumbotron\">\n",
            "<h1>E-commerce training site</h1>\n",
            "<p>\n",
            "\t\t\tWelcome to WebScraper e-commerce site. You can use this site for training\n",
            "\t\t\tto learn how to use the Web Scraper. Items listed here are not for sale.\n",
            "\t\t</p>\n",
            "</div>\n",
            "<h2>Top items being scraped right now</h2>\n",
            "<div class=\"row\">\n",
            "<div class=\"col-sm-4 col-lg-4 col-md-4\">\n",
            "<div class=\"thumbnail\">\n",
            "<img alt=\"item\" class=\"img-responsive\" src=\"/images/test-sites/e-commerce/items/cart2.png\"/>\n",
            "<div class=\"caption\">\n",
            "<h4 class=\"pull-right price\">$103.99</h4>\n",
            "<h4>\n",
            "<a class=\"title\" href=\"/test-sites/e-commerce/static/product/498\" title=\"Amazon Kindle\">Amazon Kindle</a>\n",
            "</h4>\n",
            "<p class=\"description\">6\" screen, wifi</p>\n",
            "</div>\n",
            "<div class=\"ratings\">\n",
            "<p class=\"pull-right\">3 reviews</p>\n",
            "<p data-rating=\"4\">\n",
            "<span class=\"glyphicon glyphicon-star\"></span>\n",
            "<span class=\"glyphicon glyphicon-star\"></span>\n",
            "<span class=\"glyphicon glyphicon-star\"></span>\n",
            "<span class=\"glyphicon glyphicon-star\"></span>\n",
            "</p>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"col-sm-4 col-lg-4 col-md-4\">\n",
            "<div class=\"thumbnail\">\n",
            "<img alt=\"item\" class=\"img-responsive\" src=\"/images/test-sites/e-commerce/items/cart2.png\"/>\n",
            "<div class=\"caption\">\n",
            "<h4 class=\"pull-right price\">$485.90</h4>\n",
            "<h4>\n",
            "<a class=\"title\" href=\"/test-sites/e-commerce/static/product/573\" title=\"Acer Aspire ES1-572 Black\">Acer Aspire ES1-...</a>\n",
            "</h4>\n",
            "<p class=\"description\">Acer Aspire ES1-572 Black, 15.6\" HD, Core i5-7200U, 4GB, 128GB SSD, Linux</p>\n",
            "</div>\n",
            "<div class=\"ratings\">\n",
            "<p class=\"pull-right\">6 reviews</p>\n",
            "<p data-rating=\"3\">\n",
            "<span class=\"glyphicon glyphicon-star\"></span>\n",
            "<span class=\"glyphicon glyphicon-star\"></span>\n",
            "<span class=\"glyphicon glyphicon-star\"></span>\n",
            "</p>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"col-sm-4 col-lg-4 col-md-4\">\n",
            "<div class=\"thumbnail\">\n",
            "<img alt=\"item\" class=\"img-responsive\" src=\"/images/test-sites/e-commerce/items/cart2.png\"/>\n",
            "<div class=\"caption\">\n",
            "<h4 class=\"pull-right price\">$1144.20</h4>\n",
            "<h4>\n",
            "<a class=\"title\" href=\"/test-sites/e-commerce/static/product/596\" title=\"Dell Inspiron 15 (7567) Black\">Dell Inspiron 15...</a>\n",
            "</h4>\n",
            "<p class=\"description\">Dell Inspiron 15 (7567) Black, 15.6\" FHD, Core i7-7700HQ, 8GB, 1TB, GeForce GTX 1050 Ti 4GB, Linux + Windows 10 Home</p>\n",
            "</div>\n",
            "<div class=\"ratings\">\n",
            "<p class=\"pull-right\">2 reviews</p>\n",
            "<p data-rating=\"1\">\n",
            "<span class=\"glyphicon glyphicon-star\"></span>\n",
            "</p>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"clearfix\"></div>\n",
            "<div class=\"push\"></div>\n",
            "</div>\n",
            "<div class=\"container-fluid footer\" id=\"layout-footer\">\n",
            "<div class=\"container\">\n",
            "<div class=\"row\">\n",
            "<div class=\"col-md-3\">\n",
            "<ul>\n",
            "<li><p>Products</p></li>\n",
            "<li>\n",
            "<a href=\"/\">Web Scraper browser extension</a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"/pricing\">Web Scraper Cloud</a>\n",
            "</li>\n",
            "</ul>\n",
            "</div>\n",
            "<div class=\"col-md-3\">\n",
            "<ul>\n",
            "<li><p>Company</p></li>\n",
            "<li><a href=\"/contact\">Contact</a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"/privacy-policy\">Website Privacy Policy</a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"/extension-privacy-policy\">Browser Extension Privacy Policy</a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"http://webscraperio.us-east-1.elasticbeanstalk.com/downloads/Web_Scraper_Media_Kit.zip\">Media kit</a>\n",
            "</li>\n",
            "<li><a href=\"/jobs\">Jobs</a></li>\n",
            "</ul>\n",
            "</div>\n",
            "<div class=\"col-md-3\">\n",
            "<ul>\n",
            "<li><p>Resources</p></li>\n",
            "<li><a href=\"/blog\">Blog</a></li>\n",
            "<li>\n",
            "<a href=\"/documentation\">Documentation</a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"/tutorials\">Video Tutorials</a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"/screenshots\">Screenshots</a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"/test-sites\">Test Sites</a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"https://forum.webscraper.io/\" rel=\"noopener\" target=\"_blank\">Forum</a>\n",
            "</li>\n",
            "</ul>\n",
            "</div>\n",
            "<div class=\"col-md-3\">\n",
            "<ul>\n",
            "<li><p>CONTACT US</p></li>\n",
            "<li>\n",
            "<a href=\"mailto:info@webscraper.io\">info@webscraper.io</a>\n",
            "</li>\n",
            "<li>Rupniecibas iela 30,<br/> Riga, Latvia, LV-1045</li>\n",
            "</ul>\n",
            "<ul class=\"smedia\">\n",
            "<li>\n",
            "<a href=\"https://www.facebook.com/webscraperio/\" rel=\"noopener\" target=\"_blank\"><img alt=\"Web Scraper on Facebook\" src=\"/img/fbicon.png\"/></a>\n",
            "</li>\n",
            "<li>\n",
            "<a href=\"https://twitter.com/webscraperio\" rel=\"noopener\" target=\"_blank\"><img alt=\"Web Scraper on Twitter\" src=\"/img/twicon.png\"/></a>\n",
            "</li>\n",
            "</ul>\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"row\">\n",
            "<div class=\"col-md-12\">\n",
            "<p class=\"copyright\">Copyright © 2022\n",
            "\t\t\t\t\t<a href=\"#\">Web Scraper</a> | All rights\n",
            "\t\t\t\t\treserved | Made by zoom59</p>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "</body>\n",
            "</html>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HTML Tags & Attributes\n",
        "\n",
        "Let's break down some of the key ideas of what's otherwise a intimidatingly long and complex jumble of HTML.\n",
        "\n",
        "- **Tags** are used to represent different elements such as `<title>`, `<div>` for a section within a document, and `<h1>` for the first header. \n",
        "- HTML tags usually require a closing tag such as\n",
        "HTML and will include additional information within a given tag that are known as **attributes**.\n",
        "- Attributes are marked by key words such as `class`, `id`, or `href` (referring to hyperlinks) followed by an equal sign and a description such as `<div class='container'>`\n",
        "- Our created Beautiful Soup parsed HTML object hosts a variety of methods that allows us to look at specific tags within the HTML. The following retrieves the first specified instance of the `<p>` paragraph tag in the HTML file:"
      ],
      "metadata": {
        "id": "u2LF-NeB49ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(bs_html.p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxcHI4fOftY2",
        "outputId": "0d2e19f5-31ef-4fc1-8ef4-164ac043c7ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<p>Web Scraper</p>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attributes are used to distinguish different classes of the same HTML tag, which facilitates the easy retrieval of distinct HTML elements while scraping that would otherwise have the same structure to each other. An example of this is the e-commerce test site `<div class=\"caption\">` and `<div class=\"ratings\">` to differentiate between the product descriptions and the ratings of the items being displayed on the website. \n",
        "\n",
        "We can use the `find_all` method to identify any of the `div` tags with their `class` attribute specified as a `caption`:"
      ],
      "metadata": {
        "id": "rIz4C_C_5cVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs_html.find_all('div', {'class':'caption'})"
      ],
      "metadata": {
        "id": "uo0jQz2m5oAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0edfa68a-45c8-49a4-8029-2c808ef639cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<div class=\"caption\">\n",
              " <h4 class=\"pull-right price\">$24.99</h4>\n",
              " <h4>\n",
              " <a class=\"title\" href=\"/test-sites/e-commerce/static/product/486\" title=\"Nokia 123\">Nokia 123</a>\n",
              " </h4>\n",
              " <p class=\"description\">7 day battery</p>\n",
              " </div>, <div class=\"caption\">\n",
              " <h4 class=\"pull-right price\">$409.63</h4>\n",
              " <h4>\n",
              " <a class=\"title\" href=\"/test-sites/e-commerce/static/product/559\" title=\"Lenovo V110-15ISK\">Lenovo V110-15IS...</a>\n",
              " </h4>\n",
              " <p class=\"description\">Lenovo V110-15ISK, 15.6\" HD, Core i3-6006U, 8GB, 128GB SSD, Windows 10 Home</p>\n",
              " </div>, <div class=\"caption\">\n",
              " <h4 class=\"pull-right price\">$488.64</h4>\n",
              " <h4>\n",
              " <a class=\"title\" href=\"/test-sites/e-commerce/static/product/575\" title=\"Acer Swift 1 SF113-31 Silver\">Acer Swift 1 SF1...</a>\n",
              " </h4>\n",
              " <p class=\"description\">Acer Swift 1 SF113-31 Silver, 13.3\" FHD, Pentium N4200, 4GB, 128GB SSD, Windows 10 Home</p>\n",
              " </div>]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `get_text` method strips away the HTML tags to just express the text itself:"
      ],
      "metadata": {
        "id": "-dplJWoH5o_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "captions = bs_html.find_all('div', {'class':'caption'})\n",
        "for name in captions:\n",
        "    print(name.get_text())"
      ],
      "metadata": {
        "id": "SEfOgp4M48wO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df25401f-a83c-4322-9bbb-ac1121d96996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "$103.99\n",
            "\n",
            "Amazon Kindle\n",
            "\n",
            "6\" screen, wifi\n",
            "\n",
            "\n",
            "$485.90\n",
            "\n",
            "Acer Aspire ES1-...\n",
            "\n",
            "Acer Aspire ES1-572 Black, 15.6\" HD, Core i5-7200U, 4GB, 128GB SSD, Linux\n",
            "\n",
            "\n",
            "$1144.20\n",
            "\n",
            "Dell Inspiron 15...\n",
            "\n",
            "Dell Inspiron 15 (7567) Black, 15.6\" FHD, Core i7-7700HQ, 8GB, 1TB, GeForce GTX 1050 Ti 4GB, Linux + Windows 10 Home\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic structure behind using Beautiful Soups' `find` and `find_all` methods is to first specify the tag you're interested in within the HTML document, and then the associated attributes that further narrows down the tag you're interested in. You can build from this to retrieve multiple tag types at once, such as all of the headers set to the 1, 2, and 4 subheader sizes as follows:  "
      ],
      "metadata": {
        "id": "UT2aZMv75wGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs_html.find_all(['h1','h2','h4'])"
      ],
      "metadata": {
        "id": "95J5bfIf5vnG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15fe34f4-07d3-4fd8-8060-de39436e8ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<h1>Test Sites</h1>,\n",
              " <h1>E-commerce training site</h1>,\n",
              " <h2>Top items being scraped right now</h2>,\n",
              " <h4 class=\"pull-right price\">$103.99</h4>,\n",
              " <h4>\n",
              " <a class=\"title\" href=\"/test-sites/e-commerce/static/product/498\" title=\"Amazon Kindle\">Amazon Kindle</a>\n",
              " </h4>,\n",
              " <h4 class=\"pull-right price\">$485.90</h4>,\n",
              " <h4>\n",
              " <a class=\"title\" href=\"/test-sites/e-commerce/static/product/573\" title=\"Acer Aspire ES1-572 Black\">Acer Aspire ES1-...</a>\n",
              " </h4>,\n",
              " <h4 class=\"pull-right price\">$1144.20</h4>,\n",
              " <h4>\n",
              " <a class=\"title\" href=\"/test-sites/e-commerce/static/product/596\" title=\"Dell Inspiron 15 (7567) Black\">Dell Inspiron 15...</a>\n",
              " </h4>]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can pass both the caption and ratings attributes as values in a class dictionary to identify multiple attributes in one call. "
      ],
      "metadata": {
        "id": "kMORXk7f58xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caption_reviews = bs_html.find_all('div', {'class':{'caption', 'ratings'}})\n",
        "for name in caption_reviews:\n",
        "    print(name.get_text())"
      ],
      "metadata": {
        "id": "1pJ0xQro6DCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953671a1-0fb6-4f9a-d279-cf5ef78f898b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "$103.99\n",
            "\n",
            "Amazon Kindle\n",
            "\n",
            "6\" screen, wifi\n",
            "\n",
            "\n",
            "3 reviews\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "$485.90\n",
            "\n",
            "Acer Aspire ES1-...\n",
            "\n",
            "Acer Aspire ES1-572 Black, 15.6\" HD, Core i5-7200U, 4GB, 128GB SSD, Linux\n",
            "\n",
            "\n",
            "6 reviews\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "$1144.20\n",
            "\n",
            "Dell Inspiron 15...\n",
            "\n",
            "Dell Inspiron 15 (7567) Black, 15.6\" FHD, Core i7-7700HQ, 8GB, 1TB, GeForce GTX 1050 Ti 4GB, Linux + Windows 10 Home\n",
            "\n",
            "\n",
            "2 reviews\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you look closely, you'll notice how the values for the electronic goods returned from Beautiful Soup don't actually align with what you currently see when looking at the site directly via Inspect. That's a sign that we're dealing with dynamically loaded content from Javascript, as it appears that the site is updating the items shown on the homepage outside of default values set within the HTML. We don't have the tools in our web scraping kit to handle this yet, but we'll be getting there shortly! "
      ],
      "metadata": {
        "id": "pCYAdzp06Fe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Scraped Data \n",
        "\n",
        "After identifying the tags and attributes that will allow us to match with the specific data we're interested in collecting from the website's HTML, we'd then proceed with the actual data collection within our scraper. The `pandas` library naturally intergrates with Beautiful Soup HTML parsing by structuring different tag retrievals into designated data columns. \n",
        "\n",
        "Let's therefore create a function that retrieves the product data we're interested in via the e-commerce site's URL and stores it within a Pandas DataFrame for us. `html_to_pandas` uses a dictionary that specifies the column names as keys and the collected data from the webpage as row values stored within lists. \n",
        "\n",
        "Creating each of the columns involve a different Beautiful Soup pattern matching of the HTML tag that has the data we're interested in with the tag's unique attributes. I've gone ahead and identified within the HTML itself what exactly said tag and attributes are for the title, description, price, and reviews on the site's home page. "
      ],
      "metadata": {
        "id": "L7_zuZsp67qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def html_to_pandas(url): \n",
        "    df_dict = {}\n",
        "\n",
        "    html = urlopen(url)\n",
        "    bs_html = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    df_dict['Title'] = [values.get_text() for values in bs_html.find_all('a', {'class': 'title'})]\n",
        "    df_dict['Description'] = [values.get_text() for values in bs_html.find_all('p', {'class': 'description'})]\n",
        "    df_dict['Price'] = [values.get_text() for values in bs_html.find_all('h4', {'class': 'pull-right price'})]\n",
        "    df_dict['Reviews'] = [values.get_text() for values in bs_html.find_all('p', {'class': 'pull-right'})]\n",
        "\n",
        "    product_df = pd.DataFrame(df_dict)\n",
        "    return product_df"
      ],
      "metadata": {
        "id": "YxqZad667BJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_df = html_to_pandas('https://webscraper.io/test-sites/e-commerce/static')\n",
        "product_df.head()"
      ],
      "metadata": {
        "id": "YuVb10uZ7UUK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "6a54d980-9387-4516-91b2-917897115cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Title                                        Description  \\\n",
              "0  Dell Latitude 55...  Dell Latitude 5580, 15.6\" FHD, Core i5-7300U, ...   \n",
              "1  Asus VivoBook E5...  Asus VivoBook E502NA-GO022T Dark Blue, 15.6\" H...   \n",
              "2      MSI GL62VR 7RFX  MSI GL62VR 7RFX, 15.6\" FHD, Core i7-7700HQ, 8G...   \n",
              "\n",
              "      Price    Reviews  \n",
              "0  $1178.19  6 reviews  \n",
              "1   $399.99  3 reviews  \n",
              "2  $1299.00  1 reviews  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-026f51ea-cf0b-4524-9aa1-cf32036c0f09\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>Price</th>\n",
              "      <th>Reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dell Latitude 55...</td>\n",
              "      <td>Dell Latitude 5580, 15.6\" FHD, Core i5-7300U, ...</td>\n",
              "      <td>$1178.19</td>\n",
              "      <td>6 reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Asus VivoBook E5...</td>\n",
              "      <td>Asus VivoBook E502NA-GO022T Dark Blue, 15.6\" H...</td>\n",
              "      <td>$399.99</td>\n",
              "      <td>3 reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MSI GL62VR 7RFX</td>\n",
              "      <td>MSI GL62VR 7RFX, 15.6\" FHD, Core i7-7700HQ, 8G...</td>\n",
              "      <td>$1299.00</td>\n",
              "      <td>1 reviews</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-026f51ea-cf0b-4524-9aa1-cf32036c0f09')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-026f51ea-cf0b-4524-9aa1-cf32036c0f09 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-026f51ea-cf0b-4524-9aa1-cf32036c0f09');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've successfully gone from a whole jumble of HTML and website text to a ordered DataFrame with the exact information we're interested in via Beautiful Soup. "
      ],
      "metadata": {
        "id": "I73Oy40K7aBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regular Expressions (Regexes)\n",
        "While we retrieved all of the product prices through identifying headers specified as members of the 'pull-right price' class attribute as follows:"
      ],
      "metadata": {
        "id": "lrrEfjAt79Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs_html.find_all('h4', {'class': 'pull-right price'})"
      ],
      "metadata": {
        "id": "XXFdhfhO8zE3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf769ae9-9ec7-43eb-8d19-2e98fefaced0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<h4 class=\"pull-right price\">$103.99</h4>,\n",
              " <h4 class=\"pull-right price\">$485.90</h4>,\n",
              " <h4 class=\"pull-right price\">$1144.20</h4>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we were instead in a situation where the prices weren't clearly distinguished to belong to a specific class attribute. \n",
        "We can an use regular expressions (**regexes**) to identify variable text that has some degree of consistency with its formatting\n",
        "such as all of the numbers that follow from the '$' symbol.\n",
        "\n",
        "Regexes are particularly helpful for identifying data such as emails, phone numbers, dates, or specific file types such as '.pdf' or '.jpg'. Although a complex topic on their own right that's beyond the scope of this workshop, they're very commonly used within web scraping projects and therefore important to familirize ourselves with. \n",
        "\n",
        "I highly recommend test running any code you write that uses regexes while developing your scraper to think through edge cases in the data that a current regex version may miss. I personally use [Regex 101](https://regex101.com/) as a website to check the behavior of regexes whenever I'm using them within my own work.  \n",
        "\n",
        "We can use Python's built-in `re` library to combine regexes with Beautiful Soup. A regex that will successfully identify all instances of prices specified in dollars is `\"\\$\\d+(?:\\.\\d+)?\"`. Regex syntax is convoluted, which is partially why they can be challenging to structure correctly. Let's break down what the individual components of this regex is achieving for us:\n",
        "- `\\$` starts the match based on the dollar sign symbol as our core anchor to find the product prices.\n",
        "- `\\d+` will match with one or more digits that follow the dollar sign.\n",
        "- `(?:\\.\\d+)?` specifies an optional decimal value and any digits that follows the decimals. This structure effectively handles cases where the price is listed either with or without decimal points."
      ],
      "metadata": {
        "id": "c11m9BAZ8KWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs_html.find_all(string=re.compile(\"\\$\\d+(?:\\.\\d+)?\"))"
      ],
      "metadata": {
        "id": "D2dzoNcd8854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "798eada5-5d95-45f1-e576-8d0a82a6b1ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['$103.99', '$485.90', '$1144.20']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2- Dynamic Sites with Selenium \n",
        "The first module of the coding demo provided an overview regarding how web scraping programs are fundamentally a process of identifying how data is represented within site HTML and retrieving said information through libraries such as Beautiful Soup. However, because we can't engage directly with the HTML- or specifically the Javascript-backed dynamic features we'd like to interact with to change the data featured on the page- through the Beautiful Soup library, we were limited in our ability to guide our scraper through the site's content beyond what we initially obtain through a URL request to the e-commerce test site alone. \n",
        "\n",
        "This is a very common situation individuals interested in building web scrapers find themselves in, and is why we'll therefore bring in Selenium as our next core tool to automate our site interactions. Throughout this module we'll be exploring how to properly initiatilize a Selenium web driver browser session, how to find HTML elements within the site with Selenium's particular methods, how to interact with site content through implementing wait times and addressing site exceptions, and how to piece all of these components together to obtain a final data set of product records collected over multiple web pages. \n",
        "\n",
        "We'll need to `pip install selenium` directly since it doesn't come pre-installed in our Google Colab virtual Python environment, as well as download and install the [Chrome Web Driver](https://chromedriver.chromium.org/home) that we'll be using to run our own Chrome browser through Selenium into a local directory path our program will have access to."
      ],
      "metadata": {
        "id": "qGxBYpsT4Vmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver/usr/bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT4PaxxWgNee",
        "outputId": "46231ebc-873b-4af4-96a3-b495e3108105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 995 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 384 kB 58.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 71.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.5 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.12 which is incompatible.\u001b[0m\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [950 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,992 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,324 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,424 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,546 kB]\n",
            "Fetched 11.5 MB in 4s (2,695 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 12 not upgraded.\n",
            "Need to get 91.7 MB of archives.\n",
            "After this operation, 309 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 105.0.5195.102-0ubuntu0.18.04.1 [1,156 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 105.0.5195.102-0ubuntu0.18.04.1 [80.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 105.0.5195.102-0ubuntu0.18.04.1 [5,097 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 105.0.5195.102-0ubuntu0.18.04.1 [5,320 kB]\n",
            "Fetched 91.7 MB in 8s (12.1 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 123934 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_105.0.5195.102-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "cp: missing destination file operand after '/usr/lib/chromium-browser/chromedriver/usr/bin'\n",
            "Try 'cp --help' for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the `sys` library to explicitly set the Chrome driver's location as well as import in a suite of methods from Selenium we'll be using to build our scraper. "
      ],
      "metadata": {
        "id": "JfaYrVc6ZOp2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LZ9v3BwabSE"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a designated `Options` instance to configure our Chrome driver. Selenium won't actually run within Google Colab unless the browser is set to `--headless`, while the additional options are designed to optimize resource use and prevent browser crashes. "
      ],
      "metadata": {
        "id": "_3B12KcOax1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "options = Options()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')"
      ],
      "metadata": {
        "id": "yboF1bqTTaO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll then instantiate our Selenium-driven Chrome web browser by referring to the chromedriver executable package we've downloaded into our local directory, pass our set option parameters, and then connect to the URL of the e-commerce webscraping test site. This time we'll be collecting product information specific to the touch phone sublisting of the site, as the page features the dynamic component of having multiple pages of listed products that we toggle through by clicking on page buttons. "
      ],
      "metadata": {
        "id": "tANo3KpEa6cA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver = webdriver.Chrome('chromedriver', options=options)\n",
        "url = 'https://www.webscraper.io/test-sites/e-commerce/static/phones/touch?page=1'\n",
        "driver.get(url)"
      ],
      "metadata": {
        "id": "0astS-hXQaeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding Elements \n",
        "\n",
        "Selenium offers a range of methods to locate elements within the website's HTML. Beautiful Soup's strategy to identify specific data was through the `find` and `find_all` methods, while Selenium's equivalent is based upon the combination of the `find_elements` method of our web driver with a designated parameter set via the `By` locater class. \n",
        "\n",
        "For example, we can retrieve all of the elements on the web page with a specified class value of `caption` within the `div` HTML tag as follows: "
      ],
      "metadata": {
        "id": "CKiaC3MDWz9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for vals in driver.find_elements(By.CLASS_NAME, 'caption'):\n",
        "    print(vals.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_usILw3OnPvJ",
        "outputId": "76b00821-c241-4987-c69a-2b6b31953eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$24.99\n",
            "Nokia 123\n",
            "7 day battery\n",
            "$57.99\n",
            "LG Optimus\n",
            "3.2\" screen\n",
            "$93.99\n",
            "Samsung Galaxy\n",
            "5 mpx. Android 5.0\n",
            "$109.99\n",
            "Nokia X\n",
            "Andoid, Jolla dualboot\n",
            "$118.99\n",
            "Sony Xperia\n",
            "GPS, waterproof\n",
            "$499.99\n",
            "Ubuntu Edge\n",
            "Sapphire glass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many different attributes we can combine with `By` to find elements, such as `ID`, `NAME`, and `TAG_NAME`. [Here's a helpful guide](https://https://selenium-python.readthedocs.io/locating-elements.html) that reviews the avaliable options and example HTML.  \n",
        "\n",
        "A location method that's particularly important to highlight is Selenium's support of **XPath** locators. The XPath language is similar to regexes which we learned about in the previous module, in that it provides a greater degree of customization regarding what you'd like to match, but XPath is designed to traverse HTML documents rather than string data types with regexes.   \n",
        "\n",
        "XPaths allow us to be more specific regarding what we'd like to identify in the web page compared to most other locators. They're particularly helpful when you don't have an easy name or id attribute to retrieve the element you're interested in.  \n",
        "\n",
        "The equivalent XPath syntax to retrieve the exact same information we obtained from the product captions by matching with their class name is:"
      ],
      "metadata": {
        "id": "gyQfju8GZicQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for vals in driver.find_elements(By.XPATH, \"//div[@class='caption']\"):\n",
        "    print(vals.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5uMyLuxcyw2",
        "outputId": "91acbd8a-4a84-41be-a07c-e27153f90ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$24.99\n",
            "Nokia 123\n",
            "7 day battery\n",
            "$57.99\n",
            "LG Optimus\n",
            "3.2\" screen\n",
            "$93.99\n",
            "Samsung Galaxy\n",
            "5 mpx. Android 5.0\n",
            "$109.99\n",
            "Nokia X\n",
            "Andoid, Jolla dualboot\n",
            "$118.99\n",
            "Sony Xperia\n",
            "GPS, waterproof\n",
            "$499.99\n",
            "Ubuntu Edge\n",
            "Sapphire glass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get much more specific regarding exactly what we're interested in retrieving by using XPaths, such as the price of the third phone as follows: "
      ],
      "metadata": {
        "id": "wqs8CpZKndit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver.find_elements(By.XPATH, \"//div[@class='caption']//h4[@class='pull-right price']\")[2].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kk3nARKvpLul",
        "outputId": "2f60e6f8-f9bb-43a7-986f-29e912f5045a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'$93.99'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're only briefly considering the broad topic of XPath query writing similar to regexes given our limited time today, but it's an important concept to introduce ourselves to should you find yourself in the common situation of needing more complex element location strategies when building out a Selenium-powered web scraper. "
      ],
      "metadata": {
        "id": "yNJs6hhSxU8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interacting with Elements \n",
        "\n",
        "The above examples were only collecting the data of the first page of the site that features 6 phones, but there's actually a second page that has an additional 3 phones listed that you can access by clicking either the 2 or next arrow button on the website. This is the exact circumstance we've turned to Selenium to handle for us to coordinate clicking said buttons via our headless Chrome browser.\n",
        "\n",
        "We can use the `page-link` class name to identify the button selection options found within the HTML: "
      ],
      "metadata": {
        "id": "tor8_DG-fTg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for vals in driver.find_elements(By.CLASS_NAME, 'page-link'):\n",
        "    print(vals.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcTdKKsTzIWu",
        "outputId": "d51f0474-91cb-488b-e7ff-49bbc83949c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‹\n",
            "1\n",
            "2\n",
            "›\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most robust technique for building our scraper would be clicking on the 'Next' button rather than '2', since that'll allow us to replicate our code for pages on the website with more than 2 pages worth of listed products \n",
        "\n",
        "Specifically referencing the `rel=\"next\"` relationship parameter that distinguishes the \"next\" button is a great use case for an XPath locator:  "
      ],
      "metadata": {
        "id": "vQT8FJ8b4Qrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver.find_element(By.XPATH, \"//a[@rel='next']\").text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "a7K8Giik5yO0",
        "outputId": "69295c0f-8106-4fdb-ead6-a23ee40a4f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'›'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've found a way to identify the particular button we're interested in, it's an intuitive Selenium method to actually faciliate a user click through our headless browser. Quick heads up- you're intentionally about to see an error!"
      ],
      "metadata": {
        "id": "-Y6JN7jy6WxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver.find_element(By.XPATH, \"//a[@rel='next']\").click()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "YVICtRl25rcL",
        "outputId": "3515e9d6-9187-40eb-8332-2ea5dda29cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ElementClickInterceptedException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mElementClickInterceptedException\u001b[0m          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-258c9f3b742e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"//a[@rel='next']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/webelement.py\u001b[0m in \u001b[0;36mclick\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;34m\"\"\"Clicks the element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLICK_ELEMENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWebElement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    431\u001b[0m                 response.get('value', None))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mElementClickInterceptedException\u001b[0m: Message: element click intercepted: Element <a class=\"page-link\" href=\"/test-sites/e-commerce/static/phones/touch?page=2\" rel=\"next\" aria-label=\"Next »\">...</a> is not clickable at point (139, 583). Other element would receive the click: <div class=\"acceptContainer\">...</div>\n  (Session info: headless chrome=105.0.5195.102)\nStacktrace:\n#0 0x56532ed0b1a3 <unknown>\n#1 0x56532ead7ac3 <unknown>\n#2 0x56532eb16ff1 <unknown>\n#3 0x56532eb14bc4 <unknown>\n#4 0x56532eb1219e <unknown>\n#5 0x56532eb10ff8 <unknown>\n#6 0x56532eb056fd <unknown>\n#7 0x56532eb2d072 <unknown>\n#8 0x56532eb04ee6 <unknown>\n#9 0x56532eb2d49e <unknown>\n#10 0x56532eb413cc <unknown>\n#11 0x56532eb2d443 <unknown>\n#12 0x56532eb0348b <unknown>\n#13 0x56532eb04975 <unknown>\n#14 0x56532ed872a0 <unknown>\n#15 0x56532ed4281a <unknown>\n#16 0x56532ed4234a <unknown>\n#17 0x56532ed42e75 <unknown>\n#18 0x56532ed8314b <unknown>\n#19 0x56532ed431de <unknown>\n#20 0x56532ed24c23 <unknown>\n#21 0x56532ed4d6b8 <unknown>\n#22 0x56532ed4d852 <unknown>\n#23 0x56532ed66b60 <unknown>\n#24 0x7ff2c53f76db <unknown>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're encountering an unexpected raised exception here, which is liable to occur\n",
        "whenever an interaction with a website via Selenium doesn't achieve the behavior the code developer was likely aiming to create. Exception messages are often quite informative towards identifying the underlying issue. The `ElemenentClickInterceptedException` refers to there being an \"Other element would receive the click\", and then references an `acceptContainer` div class attribute that's also within the website's HTML structure. \n",
        "\n",
        "If we switch over to the website's Inspect tab and identify what exactly the `acceptContainer` is on the website itself, we see it's equivalent to the \"We use cookies to make your Web Scraper experience better\" pop-up that appears at the bottom of the site the first time you visit the page. \n",
        "\n",
        "Although it's easy enough for us to ignore the pop-up and click on other components of the site as manual users, the pop-up is intercepting our attempted click on the next arrow button when we try to interact with the site via Selenium. The issue is simply resolved by the following:"
      ],
      "metadata": {
        "id": "0PXdrUUhIl7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver.find_element(By.CLASS_NAME, 'acceptContainer').click()"
      ],
      "metadata": {
        "id": "-MMJgB979QAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver.find_element(By.XPATH, \"//a[@rel='next']\").click()"
      ],
      "metadata": {
        "id": "nMFXEplDJiRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a great example of a common situation with building scrapers for dynamic sites, in that you often can't tell what may raise issues within your code until you're actually testing with a web driver.\n",
        "\n",
        "What happen if we were to click the \"next\" button a second time? Remember that we only have two pages worth of listed phones, so we're about to see a different type of exception come up: "
      ],
      "metadata": {
        "id": "33rc3cEkJlbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver.find_element(By.XPATH, \"//a[@rel='next']\").click()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "Bt8YM62BJwkX",
        "outputId": "a3ed3ae5-3b77-4725-b671-56dbd928875e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NoSuchElementException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-258c9f3b742e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"//a[@rel='next']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    856\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[1;32m    857\u001b[0m             \u001b[0;34m'using'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m             'value': value})['value']\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWebElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    431\u001b[0m                 response.get('value', None))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//a[@rel='next']\"}\n  (Session info: headless chrome=105.0.5195.102)\nStacktrace:\n#0 0x56532ed0b1a3 <unknown>\n#1 0x56532ead7ac3 <unknown>\n#2 0x56532eb0f869 <unknown>\n#3 0x56532eb0fa41 <unknown>\n#4 0x56532eb436e7 <unknown>\n#5 0x56532eb2d10d <unknown>\n#6 0x56532eb413cc <unknown>\n#7 0x56532eb2d443 <unknown>\n#8 0x56532eb0348b <unknown>\n#9 0x56532eb04975 <unknown>\n#10 0x56532ed872a0 <unknown>\n#11 0x56532ed4281a <unknown>\n#12 0x56532ed4234a <unknown>\n#13 0x56532ed42e75 <unknown>\n#14 0x56532ed8314b <unknown>\n#15 0x56532ed431de <unknown>\n#16 0x56532ed24c23 <unknown>\n#17 0x56532ed4d6b8 <unknown>\n#18 0x56532ed4d852 <unknown>\n#19 0x56532ed66b60 <unknown>\n#20 0x7ff2c53f76db <unknown>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The raised `NoSuchElementException` implies that our program doesn't think that the next arrow button exists. If you dig into the HTML on the website itself via Inspect following clicking the next arrow button, you can see that the website updates to list the button as a member of the class attibute `\"page-item disabled\"` subtree within the pagination container of the site. This means that the site is recognizing that there are no more pages to click the next arrow following one click to the second page. \n",
        "\n",
        "Receiving this error reasserts for us that our Selenium-powered web driver session is indeed similar to us manually interacting with the site itself, as our past behaviors during our entire scraping workflow dictate how the website changes and therefore how we should adapt our code to accomadate the new site structure. \n",
        "\n",
        "We can therefore adjust by prompting our program to click the previous arrow button instead, which has switched from `\"page-item disabled\"` class status to now a live page link after we guided our driver to move from the first to second page: "
      ],
      "metadata": {
        "id": "ZWQQb4gcKSIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver.find_element(By.XPATH, \"//a[@rel='prev']\").click()"
      ],
      "metadata": {
        "id": "tTmlbSJ9fb9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This puts us back to where we started on the first page of the listed phones for sale. "
      ],
      "metadata": {
        "id": "hTCoupTbDfK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Waits\n",
        "\n",
        "Now that we've covered how to interact directly with the site through clicking elements as well as a few common complications within the click-through process to keep an eye out for, we're ready to move towards using our program to properly time the transitions between site interactions, website updates, and then correctly identifying newly displayed data.  \n",
        "\n",
        "Waits, website element checks, and managing exceptions go hand-in-hand with each other. By implementing waits, we avoid errors in our scraper by allotting the time needed for the website to properly update. Selenium offers two types of waits which we'll both consider towards the goal of retrieving all of the phone names on the e-commerce site across both of the pages. "
      ],
      "metadata": {
        "id": "-MBocjmxfYFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implicit waits** sets a designated amount of time that the web driver will pause for before raising an exception. They're designed to broadly estimate how long a website will take to update following clicking on a page. \n",
        "\n",
        "The following function combines multiple of our previous steps within the module to collect the phone names from the first page of the site, click on the cookie pop-up and next arrow button, use a 5-second wait to give the site time to update following the next arrow click, and then print out all 9 phone names that we're expecting across the two pages. It also initializes a new connection with the site via the web driver each time it's called so we won't have to worry about exceptions caused by prior button clicks each time we run the function: \n"
      ],
      "metadata": {
        "id": "rVdBo8b8lYj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def implicit_wait(url): \n",
        "    driver = webdriver.Chrome('chromedriver', options=options)\n",
        "    driver.get(url)\n",
        "\n",
        "    element_list = []\n",
        "\n",
        "    for vals in driver.find_elements(By.XPATH, \"//a[@class='title']\"):\n",
        "        element_list.append(vals.text)\n",
        "    driver.find_element(By.CLASS_NAME, 'acceptContainer').click()\n",
        "    driver.find_element(By.XPATH, \"//a[@rel='next']\").click()\n",
        "    driver.implicitly_wait(5)\n",
        "    for vals in driver.find_elements(By.XPATH, \"//a[@class='title']\"):\n",
        "        element_list.append(vals.text)\n",
        "\n",
        "    return print(*element_list, sep='\\n')\n",
        "\n",
        "implicit_wait(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LzGvykNldsE",
        "outputId": "eadce1fb-81c2-4698-e17c-81a36b33ae00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nokia 123\n",
            "LG Optimus\n",
            "Samsung Galaxy\n",
            "Nokia X\n",
            "Sony Xperia\n",
            "Ubuntu Edge\n",
            "Iphone\n",
            "Iphone\n",
            "Iphone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Success! While implicit waits are a solid technique and certainly important to be familiar with when using Selenium, I'd overall recommend using **explicit waits** over implicit waits when given the option between the two. Explicit waits employ both the specified pause time of implicit waits with the additional feature of designating conditions that Selenium will check whether they become satisfied after interacting with the web site. It therefore provides more information regarding what you're expecting to occur following interacting with the page beyond just placing a brief wait without any additional context. \n",
        "\n",
        "Explicit waits are conducted via the `WebDriverWait` class in combination with `expected_conditions` (commonly shortened to `EC`). The one-line adaption of our implicit wait code block via the `explicit_wait` function that follows below designates what the browser will need to locate before proceeding forward with collecting data from the second page. \n",
        "\n",
        "`EC.presence_of_element_located` specifies our web driver to pause until the titles of the phones on the second page are displayed on the website. The number you pass along with `WebDriverWait` sets the amount of time before raising an exception similar to implicit waits, but this time it's based on whether the expected condition is met within the updated website HTML. I often use `EC.element_to_be_clickable` as well to ensure that a new element is fully loaded for interaction as my expected condition parameter, while many more conditions are available [as listed here](https://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions). "
      ],
      "metadata": {
        "id": "7gmio6bdyjsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def explicit_wait(url): \n",
        "    driver = webdriver.Chrome('chromedriver', options=options)\n",
        "    driver.get(url)\n",
        "\n",
        "    element_list = []\n",
        "\n",
        "    for vals in driver.find_elements(By.XPATH, \"//a[@class='title']\"):\n",
        "        element_list.append(vals.text)\n",
        "    driver.find_element(By.CLASS_NAME, 'acceptContainer').click()\n",
        "    driver.find_element(By.XPATH, \"//a[@rel='next']\").click()\n",
        "    WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH , \"//a[@class='title']\")))\n",
        "    for vals in driver.find_elements(By.XPATH, \"//a[@class='title']\"):\n",
        "        element_list.append(vals.text)\n",
        "\n",
        "    return print(*element_list, sep='\\n')\n",
        "\n",
        "explicit_wait(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzXIvlnF06Sq",
        "outputId": "6285a5e7-26ee-4233-8d03-fc2d676c20aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nokia 123\n",
            "LG Optimus\n",
            "Samsung Galaxy\n",
            "Nokia X\n",
            "Sony Xperia\n",
            "Ubuntu Edge\n",
            "Iphone\n",
            "Iphone\n",
            "Iphone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining into a Pandas DataFrame\n",
        "\n",
        "Let's finish by combining all of the core ideas we've covered to collect the prices, titles, descriptions, and number of reviews of all of the phones on the two pages of the e-commerce site. We'll know that we've succeeded if we obtain a DataFrame with 9 unique records as that's the total count of unique items listed across the two pages. \n",
        "\n",
        "The following `phone_dataframe` function merges all of our previously covered topics into one code block. We first initiatize our web driver, connect to the e-commerce site URL, and take care of the cookie pop-up that would otherwise conflict with our page button clicks. We additionally create a data list to collect the record values for each product. \n",
        "\n",
        "The `max_pages` variable is designed to serve as a stopping condition so Selenium will know when we've reached the last page and it can therefore break out of the data collection portion of the function. The complete workflow of data collection is based on the condition of `max_pages` exisiting, where for each round of data scraping we subsequently reduce the value by one. The idea behind this approach is that you can change `max_pages` to represent the number of pages you'd like to collect data from as needed. \n",
        "\n",
        "The text associated with the `thumbnail` class parameter in the HTML contains the exact four data points for each record that we're interested in, so we use an XPath based on identifying the `thumbnail` attribute as our element finder. You'll also note that our clicking of the next arrow button and then explicit wait is conditioned on the `max_pages` parameter not being at 1. This is the avoid the raised exception scenario we reviewed earlier regarding the next arrow being disabled by the site once we've reached the last page, which is equivalent to 1 in our `max_pages` condition. "
      ],
      "metadata": {
        "id": "x6mNFu_5gEKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def phone_dataframe(url): \n",
        "    driver = webdriver.Chrome('chromedriver', options=options)\n",
        "    driver.get(url)\n",
        "    driver.find_element(By.CLASS_NAME, 'acceptContainer').click()\n",
        "\n",
        "    data_list = []\n",
        "    max_pages = 2\n",
        "\n",
        "    while max_pages: \n",
        "        for vals in driver.find_elements(By.XPATH, \"//div[@class='thumbnail']\"):\n",
        "            vals = re.split(r'\\n', vals.text)\n",
        "            data_list.append(vals)\n",
        "        \n",
        "        if max_pages != 1: \n",
        "            driver.find_element(By.XPATH, \"//a[@rel='next']\").click()\n",
        "        WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH , \"//a[@rel='prev']\")))\n",
        "        max_pages -= 1\n",
        "\n",
        "    phone_df = pd.DataFrame(data_list)\n",
        "    phone_df.columns = ['Price', 'Title', 'Description', 'Reviews']\n",
        "    return phone_df"
      ],
      "metadata": {
        "id": "uu_813TME2cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phone_df = phone_dataframe(url)\n",
        "phone_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "YsTEPazpBnxT",
        "outputId": "07b7558f-3a7c-4221-f62c-feb3ae3187fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Price           Title             Description     Reviews\n",
              "0   $24.99       Nokia 123           7 day battery  11 reviews\n",
              "1   $57.99      LG Optimus             3.2\" screen  11 reviews\n",
              "2   $93.99  Samsung Galaxy      5 mpx. Android 5.0   3 reviews\n",
              "3  $109.99         Nokia X  Andoid, Jolla dualboot   4 reviews\n",
              "4  $118.99     Sony Xperia         GPS, waterproof   6 reviews\n",
              "5  $499.99     Ubuntu Edge          Sapphire glass   2 reviews\n",
              "6  $899.99          Iphone                   White  10 reviews\n",
              "7  $899.99          Iphone                  Silver   8 reviews\n",
              "8  $899.99          Iphone                   Black   1 reviews"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0745490b-f120-437d-b0f2-ce2da4cdb514\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>Reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>$24.99</td>\n",
              "      <td>Nokia 123</td>\n",
              "      <td>7 day battery</td>\n",
              "      <td>11 reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>$57.99</td>\n",
              "      <td>LG Optimus</td>\n",
              "      <td>3.2\" screen</td>\n",
              "      <td>11 reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>$93.99</td>\n",
              "      <td>Samsung Galaxy</td>\n",
              "      <td>5 mpx. Android 5.0</td>\n",
              "      <td>3 reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>$109.99</td>\n",
              "      <td>Nokia X</td>\n",
              "      <td>Andoid, Jolla dualboot</td>\n",
              "      <td>4 reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>$118.99</td>\n",
              "      <td>Sony Xperia</td>\n",
              "      <td>GPS, waterproof</td>\n",
              "      <td>6 reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>$499.99</td>\n",
              "      <td>Ubuntu Edge</td>\n",
              "      <td>Sapphire glass</td>\n",
              "      <td>2 reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>$899.99</td>\n",
              "      <td>Iphone</td>\n",
              "      <td>White</td>\n",
              "      <td>10 reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>$899.99</td>\n",
              "      <td>Iphone</td>\n",
              "      <td>Silver</td>\n",
              "      <td>8 reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>$899.99</td>\n",
              "      <td>Iphone</td>\n",
              "      <td>Black</td>\n",
              "      <td>1 reviews</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0745490b-f120-437d-b0f2-ce2da4cdb514')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0745490b-f120-437d-b0f2-ce2da4cdb514 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0745490b-f120-437d-b0f2-ce2da4cdb514');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our final DataFrame does indeed feature 9 records, confirming that our complete function successfully handled clicking through multiple pages worth of product listings. \n",
        "\n",
        "Congratulations on completing the second module of our webscraping coding demos! We started with the very basics of connecting to a website and inspecting HTML code and advanced all the way to interacting directly with the site to retrieve dynamically generated data. Let's now return to the slides to conclude our web scraping workshop. "
      ],
      "metadata": {
        "id": "VQUepV7GOl6N"
      }
    }
  ]
}